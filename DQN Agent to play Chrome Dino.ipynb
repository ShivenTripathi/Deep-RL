{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4Z-KigLPgEYr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using TensorFlow backend.\n"
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from gym.spaces.box import Box\n",
        "from gym.core import Wrapper\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces import Box\n",
        "import sys\n",
        "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
        "import cv2\n",
        "sys.path.append('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
        "from keras.layers import Conv2D, Dense, Flatten\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD , Adam\n",
        "import gym_chrome_dino\n",
        "from gym_chrome_dino.utils.wrappers import make_dino\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfc\n",
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(obses_t),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(obses_tp1),\n",
        "            np.array(dones)\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxes = [\n",
        "            random.randint(0, len(self._storage) - 1)\n",
        "            for _ in range(batch_size)\n",
        "        ]\n",
        "        return self._encode_sample(idxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab_type": "text",
        "id": "f8ELf17hgEY6"
      },
      "outputs": [],
      "source": [
        "class FrameBuffer(Wrapper):\n",
        "    \n",
        "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        self.dim_order = dim_order\n",
        "        if dim_order == 'tensorflow':\n",
        "            height, width, n_channels = env.observation_space.shape\n",
        "            obs_shape = [height, width, n_channels * n_frames]\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info\n",
        "\n",
        "    def update_buffer(self, img):\n",
        "        if self.dim_order == 'tensorflow':\n",
        "            offset = self.env.observation_space.shape[-1]\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n",
        "        elif self.dim_order == 'pytorch':\n",
        "            offset = self.env.observation_space.shape[0]\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        self.framebuffer = np.concatenate(\n",
        "            [img, cropped_framebuffer], axis=axis)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXsK-IHfgEYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessgame(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        ObservationWrapper.__init__(self,env)        \n",
        "        self.img_size = (80,80)\n",
        "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
        "\n",
        "    def observation(self, img): \n",
        "            \n",
        "        img = img[:,:480,:]\n",
        "        img = cv2.resize(img, self.img_size)\n",
        "        img=img.mean(-1,keepdims=True)\n",
        "        img = img.astype('float32') / 255.\n",
        "               \n",
        "        return img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IEhyFCngEY6",
        "colab_type": "code",
        "outputId": "e73ffe09-6658-40a2-bb73-d5fba9faa1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def make_env():\n",
        "    env = gym.make('ChromeDinoNoBrowser-v0')\n",
        "    env = Preprocessgame(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hISiOTkbgEZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess=tfc.InteractiveSession()\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bw9X-S8gEZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
        "\n",
        "        with tfc.variable_scope(name, reuse=reuse):\n",
        "\n",
        "            self.network = Sequential()\n",
        "            self.network.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=state_shape))  \n",
        "            self.network.add(MaxPooling2D(pool_size=(2,2)))\n",
        "            self.network.add(Activation('relu'))\n",
        "            self.network.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
        "            self.network.add(MaxPooling2D(pool_size=(2,2)))\n",
        "            self.network.add(Activation('relu'))\n",
        "            self.network.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
        "            self.network.add(MaxPooling2D(pool_size=(2,2)))\n",
        "            self.network.add(Activation('relu'))\n",
        "            self.network.add(Flatten())\n",
        "            self.network.add(Dense(512))\n",
        "            self.network.add(Activation('relu'))\n",
        "            self.network.add(Dense(n_actions))\n",
        "            adam = Adam(lr=10**-5)\n",
        "            self.network.compile(loss='mse',optimizer=adam)\n",
        "\n",
        "            self.state_t = tfc.placeholder('float32', [None, ] + list(state_shape))\n",
        "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
        "\n",
        "        self.weights = tfc.get_collection(\n",
        "            tfc.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_symbolic_qvalues(self, state_t):\n",
        "        qvalues = self.network(state_t)\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, state_t):\n",
        "        sess = tfc.get_default_session()\n",
        "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUMfQPigEZO",
        "colab_type": "code",
        "outputId": "228bc25d-d158-4052-90c5-1c1537799b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)\n",
        "sess.run(tfc.global_variables_initializer())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:From /home/shiven/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1bFIadgEZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
        "    s = env.framebuffer\n",
        "    reward = 0.0\n",
        "    for t in range(n_steps):\n",
        "        qvalues = agent.get_qvalues([s])\n",
        "        action = agent.sample_actions(qvalues)[0]\n",
        "        next_s, r, done, _ = env.step(action)\n",
        "        exp_replay.add(s, action, r, next_s, done)\n",
        "        reward += r\n",
        "        if done:\n",
        "            s = env.reset()\n",
        "        else:\n",
        "            s = next_s\n",
        "    return reward"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def player(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    rewards = []\n",
        "    scores=[]\n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        \n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "            reward += r\n",
        "            if done:\n",
        "                scores.append(env.get_score())\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    print(np.mean(scores))\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3E5EPwugEZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT_PV5dWgEZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_weigths_into_target_network(agent, target_network):\n",
        "    assigns = []\n",
        "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
        "        assigns.append(tfc.assign(w_target, w_agent, validate_shape=True))\n",
        "    return assigns"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvG9k_bOgEZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs_ph = tfc.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "actions_ph = tfc.placeholder(tf.int32, shape=[None])\n",
        "rewards_ph = tfc.placeholder(tf.float32, shape=[None])\n",
        "next_obs_ph = tfc.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "is_done_ph = tfc.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "is_not_done = 1 - is_done_ph\n",
        "gamma = 0.99"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMcRAXOcgEZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
        "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9uTxmRvgEaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph)\n",
        "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
        "reference_qvalues = rewards_ph + gamma*next_state_values_target*is_not_done\n",
        "td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
        "td_loss = tf.reduce_mean(td_loss)\n",
        "train_step = tfc.train.AdamOptimizer(1e-3).minimize(td_loss, var_list=agent.weights)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l27qIxh4gEaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.run(tfc.global_variables_initializer())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H0Hz7MVgEaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_average(x, span=100, **kw):\n",
        "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
        "\n",
        "mean_rw_history = []\n",
        "td_loss_history = []"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq1CmxRggEaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_replay = ReplayBuffer(10**2)\n",
        "play_and_record(agent, env, exp_replay, n_steps=10**1)\n",
        "def sample_batch(exp_replay, batch_size):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
        "    return {\n",
        "        obs_ph: obs_batch,\n",
        "        actions_ph: act_batch,\n",
        "        rewards_ph: reward_batch,\n",
        "        next_obs_ph: next_obs_batch,\n",
        "        is_done_ph: is_done_batch,\n",
        "    }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESgYz_t4gEaY",
        "colab_type": "code",
        "outputId": "08bc4a49-4592-4545-d374-aad82f436e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "for i in trange(10**2*2):\n",
        "    # play\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "\n",
        "    # train\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "\n",
        "    # adjust agent parameters\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[12, 4])\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "agent.network.save_weights('model.h5')\n",
        "print(\"We finish building the model\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.network.load_weights(\"model.h5\")\n",
        "from keras.optimizers import SGD , Adam\n",
        "adam = Adam(lr=10**-4)\n",
        "agent.network.compile(loss='mse',optimizer=adam)\n",
        "print(agent.epsilon)\n",
        "print (\"Weight load successfully\")\n",
        "agent.epsilon=0\n",
        "print(agent.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores=[]\n",
        "def play_and_record2(agent, env, n_steps=1):\n",
        "    s = env.reset()\n",
        "    reward = 0.0\n",
        "    for t in range(n_steps):\n",
        "        \n",
        "        qvalues = agent.get_qvalues([s])\n",
        "        action = agent.sample_actions(qvalues)[0]\n",
        "        print(action)\n",
        "        next_s, r, done, _ = env.step(action)\n",
        "        exp_replay.add(s, action, r, next_s, done)\n",
        "        reward += r\n",
        "        if done:\n",
        "            score=env.get_score()\n",
        "            scores.append(score)\n",
        "            break\n",
        "        else:\n",
        "            s = next_s\n",
        "    return scores\n",
        "def make_env2():\n",
        "    env = gym.make('ChromeDino-v0')\n",
        "    env = Preprocessgame(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(agent.epsilon)\n",
        "player(make_env2(),agent,n_games=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of dqn_atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python35664bittryruncondab95fb79d8757413d8f8d35d52e41ba00",
      "display_name": "Python 3.5.6 64-bit ('try_run': conda)"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
